{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfa52162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "import re\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7beb84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a database connection using pymysql\n",
    "def create_connection():\n",
    "    return pymysql.connect(host='database-1.csopvl4k4p5e.us-east-1.rds.amazonaws.com',\n",
    "                           user='admin',\n",
    "                           password='llmtest123',\n",
    "                           database='LLMProject',\n",
    "                           port=3306,\n",
    "                           charset='utf8mb4',\n",
    "                           cursorclass=pymysql.cursors.DictCursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1364b275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Normalize text by encoding to ASCII and decoding back to string\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    # Remove non-alphanumeric characters and unnecessary spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s,.?!]', '', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6be16c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load and clean the dataset\n",
    "def load_and_clean_data():\n",
    "    connection = create_connection()\n",
    "    df = pd.DataFrame()\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            sql = 'SELECT * FROM reddit_hn'  # Change to select all columns\n",
    "            cursor.execute(sql)\n",
    "            rows = cursor.fetchall()  # Fetch all the results\n",
    "            df = pd.DataFrame(rows)  # Convert to DataFrame\n",
    "    finally:\n",
    "        connection.close()\n",
    "\n",
    "    # Check if the DataFrame is not empty and the columns exist before cleaning\n",
    "    if not df.empty and 'SubmissionTitle' in df.columns and 'Text' in df.columns:\n",
    "        df['SubmissionTitle'] = df['SubmissionTitle'].fillna('').apply(clean_text)\n",
    "        df['Text'] = df['Text'].fillna('').apply(clean_text)\n",
    "\n",
    "        # Optionally, remove rows where 'SubmissionTitle' or 'Text' are empty strings after cleaning\n",
    "        df = df[(df['SubmissionTitle'] != '') & (df['Text'] != '')]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "558d7145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          CreatedTime SubmissionID  \\\n",
      "0 2023-04-11 23:58:09      12j2q08   \n",
      "1 2023-04-12 05:16:08      12jb0p7   \n",
      "2 2023-04-12 05:21:23      12jb52v   \n",
      "3 2023-04-12 09:51:42      12jgw3i   \n",
      "4 2023-04-12 13:29:18      12jmb5e   \n",
      "\n",
      "                                     SubmissionTitle  \\\n",
      "0  How come Chatgp3 is terrible at analyzing gram...   \n",
      "1  Just Released an OpenSource Tool to Help Test ...   \n",
      "2  Is OpenAIs Study On The Labor Market Impacts O...   \n",
      "3  How do companies tackle observability, bias, a...   \n",
      "4         Do LLMs retain information interlingually?   \n",
      "\n",
      "                                                Text  \\\n",
      "0  So I have been playing around with GP3 alot re...   \n",
      "1  I pushed out some simple code for running expe...   \n",
      "2  Example imgnamehttpspreview.redd.itsqjd5aiu1et...   \n",
      "3  Hey rGPT3 community!\\n\\nIve been diving into t...   \n",
      "4  If an LLM like GPT4 is fed information in one ...   \n",
      "\n",
      "                                       SubmissionURL  Score  NumberOfComments  \\\n",
      "0  /r/GPT3/comments/12j2q08/how_come_chatgp3_is_t...      1                 4   \n",
      "1  /r/GPT3/comments/12jb0p7/just_released_an_open...      1                 0   \n",
      "2  /r/GPT3/comments/12jb52v/is_openais_study_on_t...      1                 0   \n",
      "3  /r/GPT3/comments/12jgw3i/how_do_companies_tack...      8                 6   \n",
      "4  /r/GPT3/comments/12jmb5e/do_llms_retain_inform...      9                19   \n",
      "\n",
      "  TopicName  \n",
      "0       GPT  \n",
      "1       GPT  \n",
      "2       GPT  \n",
      "3       GPT  \n",
      "4       GPT  \n",
      "17531\n"
     ]
    }
   ],
   "source": [
    "# Load and clean the data\n",
    "cleaned_df = load_and_clean_data()\n",
    "print(cleaned_df.head())  # Display the first few rows of the cleaned dataframe\n",
    "print(cleaned_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b4322b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.2.2+cu121 with CUDA 1201 (you have 2.2.2+cpu)\n",
      "    Python  3.10.11 (you have 3.10.6)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "# Initialize the Hugging Face sentiment-analysis pipeline with RoBERTa model\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e1dd886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle long texts and classify sentiment\n",
    "def get_sentiment(text):\n",
    "    try:\n",
    "        result = sentiment_pipeline(text[:512])  # Truncate text to 512 characters\n",
    "        sentiment_label = result[0]['label']\n",
    "        sentiment_score = result[0]['score']\n",
    "\n",
    "        # Map model-specific output labels to generic labels\n",
    "        if sentiment_label == 'LABEL_0':\n",
    "            sentiment = 'NEGATIVE'\n",
    "        elif sentiment_label == 'LABEL_1':\n",
    "            sentiment = 'NEUTRAL'\n",
    "        else: \n",
    "            sentiment = 'POSITIVE'\n",
    "        \n",
    "        return sentiment, sentiment_score\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return \"ERROR\", 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a1b3a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_database(df, original_table_name, new_table_name):\n",
    "    connection = create_connection()\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            # Attempt to retrieve the structure of the original table\n",
    "            try:\n",
    "                cursor.execute(f\"SHOW CREATE TABLE {original_table_name}\")\n",
    "                create_table_query = cursor.fetchone()['Create Table']\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Error retrieving table structure: {e}\")\n",
    "\n",
    "            # Modify the table structure query to create a new table\n",
    "            create_table_query = create_table_query.replace(original_table_name, new_table_name)\n",
    "            pos = create_table_query.rfind(')')\n",
    "            create_table_query = create_table_query[:pos] + ', Sentiment VARCHAR(10), Sentiment_Score FLOAT' + create_table_query[pos:]\n",
    "\n",
    "            # Create the new table\n",
    "            try:\n",
    "                cursor.execute(f\"DROP TABLE IF EXISTS {new_table_name}\")\n",
    "                cursor.execute(create_table_query)\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Error creating new table: {e}\")\n",
    "\n",
    "            # Prepare and execute the insert statement\n",
    "            fields = list(df.columns)\n",
    "            placeholders = ', '.join(['%s'] * len(fields))\n",
    "            insert_query = f\"INSERT INTO {new_table_name} ({', '.join(fields)}) VALUES ({placeholders})\"\n",
    "#             try:\n",
    "#                 for _, row in df.iterrows():\n",
    "#                     values = tuple(row[field] if field in row else None for field in fields)\n",
    "#                     cursor.execute(insert_query, values)\n",
    "#                 connection.commit()\n",
    "#             except Exception as e:\n",
    "#                 connection.rollback()  # Rollback in case of error\n",
    "#                 raise Exception(f\"Error inserting data: {e}\")\n",
    "\n",
    "            # Prepare data for bulk insert\n",
    "            data_to_insert = [tuple(row[field] for field in fields) for index, row in df.iterrows()]\n",
    "\n",
    "            # Execute bulk insert\n",
    "            cursor.executemany(insert_query, data_to_insert)\n",
    "            \n",
    "            connection.commit()\n",
    "    except Exception as final_error:\n",
    "        print(f\"Database operation failed: {final_error}\")\n",
    "    finally:\n",
    "        connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70c7669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the sentiment analysis function to the Text column\n",
    "cleaned_df[['Sentiment', 'Sentiment_Score']] = cleaned_df['Text'].apply(lambda x: pd.Series(get_sentiment(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5380717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame with sentiment data to the new table\n",
    "save_to_database(cleaned_df, 'reddit_hn', 'sentiment_analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "314db599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65\n",
      "Precision: 0.6950000000000001\n",
      "Recall: 0.65\n",
      "F1-Score: 0.6573975044563279\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Function to load annotated CSV\n",
    "def load_annotated_data(file_path):S\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Function to predict sentiment using your existing pipeline\n",
    "def predict_sentiment(df, text_column):\n",
    "    sentiments = df[text_column].apply(lambda x: get_sentiment(x)[0])\n",
    "    return sentiments\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(predictions, annotations):\n",
    "    accuracy = accuracy_score(annotations, predictions)\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(annotations, predictions, average='weighted')\n",
    "    return accuracy, precision, recall, fscore\n",
    "\n",
    "# Load the annotated data\n",
    "annotated_data = load_annotated_data(\"sentiment_reddit_data.csv\")\n",
    "\n",
    "# Predict the sentiment\n",
    "predicted_sentiments = predict_sentiment(annotated_data, 'Text')\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy, precision, recall, fscore = evaluate_model(predicted_sentiments, annotated_data['Annotated_Sentiment'])\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {fscore}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5667768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
